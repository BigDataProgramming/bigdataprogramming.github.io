# Programming Big Data Applications: Book Exercises


This is the code repository for the book [Programming Big Data Applications](https://www.worldscientific.com/worldscibooks/10.1142/q0444),
published by [World Scientific](https://www.worldscientific.com/). It contains the code of exercices proposed
in the book.

<div style="text-align: center">
<img height="400px" style="margin: auto" src="./assets/book-cover.png">
</div>

> Talia, D., Trunfio, P., Marozzo, F., Belcastro, L., Cantini, R., & Orsino, A. (2024).
<b>Programming Big Data Applications</b>. WORLD SCIENTIFIC (EUROPE). https://doi.org/10.1142/q0444

## About the Book
In the age of the Internet of Things and social media platforms, huge amounts of digital data are generated by and
collected from many sources, including sensors, mobile devices, wearable trackers and security cameras. These data,
commonly referred to as Big Data, are challenging current storage, processing and analysis capabilities. New models,
languages, systems and algorithms continue to be developed to effectively collect, store, analyze and learn from Big
Data. Programming Big Data Applications introduces and discusses models, programming frameworks and algorithms to
process and analyze large amounts of data. In particular, the book provides an in-depth description of the properties
and mechanisms of the main programming paradigms for Big Data analysis, including MapReduce, workflow, BSP, message
passing, and SQL-like. 

Through programming examples it also describes the most used frameworks for Big Data analysis
like Hadoop, Spark, MPI, Hive, Storm and others. We discuss and compare the different systems by highlighting the main
features of each of them, their diffusion (both within their community of developers and users), and their main
advantages and disadvantages in implementing Big Data analysis applications.

### How to cite
````
@book{doi:10.1142/q0444,
    author = {Talia, Domenico and Trunfio, Paolo and Marozzo, Fabrizio and Belcastro, Loris and Cantini, Riccardo and
    Orsino, Alessio},
    title = {Programming Big Data Applications},
    publisher = {WORLD SCIENTIFIC (EUROPE)},
    year = {2024},
    doi = {10.1142/q0444},
    URL = {https://www.worldscientific.com/doi/abs/10.1142/q0444},
    eprint = {https://www.worldscientific.com/doi/pdf/10.1142/q0444}
}
````

## Project structure
To run the provided exercises on a distributed environment,
a cluster of Docker containers has been configured as follows:

- **master** node, which runs the following services: Hadoop NameNode/Resource Manager, Spark Master, Hive Server, Zookeeper Server, Airflow Scheduler/Triggerer/Webserver
- **worker** nodes, namely _worker1_ and _worker2_, which run the following services: 
Hadoop DataNode/NodeManager, Spark Worker, Storm Supervisor, Airflow Celery Worker Server 

- **history server**, which acts as the Hadoop/Spark History server to enable developers to monitor
the metrics and performance of completed Spark/Hadoop applications.
- **jupyter** node, which provides an instance of Jupyter Lab, a popular web tool providing a 
browser-based interface that allows develpers to use multiple notebooks together.
In particular, the provided instance includes two kernel, namely _ipykernel_ and _spylon-kernel_, for 
running notebook code in Scala and Python.

- **metastore** node, an instance of Postgres server acting as metastore for systems requiring DBMS for storing data (e.g., Apache Hive, Apache Airflows).

- **rabbitmq** node, an instance of RabbitMQ server, a reliable and mature messaging and streaming broker, which is used by Apache Airflow.

## Getting started
For executing this examples, you need to install Docker on your machine. 
You can download and install Docker on multiple platforms (Mac, Windows, Linux).
For more details on how to install, set up, configure, and use Docker, please refer to the 
official [guide](https://docs.docker.com/get-docker/).

After have installed Docker, you must clone or download the repository:

```bash
git clone git@gitlab.com:lorisbel/docker-bigdata-book.git
```

After downloading or cloning the repository, go into the 
project root folder and run the following commands to deploy the cluster:

```bash
make build
docker-compose up
```

> **NOTE:** The image building phase might take several minutes.

## Usage
Users are free to download and use this code. To facilitate usage, a README file has been included 
in the folder of each exercise, providing details about the code and explaining how to run it in the 
distributed environment. Additionally, each exercise folder includes a bash script, _run.sh_, 
that automates the process of building, setting up, and running the example.

### Apache Hadoop
- ResourceManager: [http://localhost:8088](http://localhost:8088)
- NameNode: [http://localhost:9870](http://localhost:9870)
- HistoryServer: [http://localhost:19888](http://localhost:19888)
- Datanode 1: [http://localhost:9864](http://localhost:9864) 
- Datanode 2: [http://localhost:9865](http://localhost:9865)
- NodeManager 1: [http://localhost:8042](http://localhost:8042) 
- NodeManager 2: [http://localhost:8043](http://localhost:8043)

### Apache Spark
- Spark Master: [http://localhost:8080](http://localhost:8080)
- Spark Worker 1: [http://localhost:8081](http://localhost:8081)
- Spark Worker 2: [http://localhost:8082](http://localhost:8082)

### Apache Hive
- Hive URI: [jdbc:hive2://localhost:10000](jdbc:hive2://localhost:10000)

### Apache Airflow
- Airflow UI: [http://localhost:8881](http://localhost:8881)

### Apache Storm
- Storm UI: [http://localhost:8089](http://localhost:8089)

### Jupyter Notebook
- URL: [http://localhost:8888/lab](http://localhost:8888/lab)


## Contributing

Feel free to contribute to this GitHub project by reporting any bugs or 
providing suggestions for further improving the code; your valuable feedback is highly 
encouraged and appreciated!

------------------------------------------------------------------------
## License
Â© Talia Domenico, Trunfio Paolo, Marozzo Fabrizio, Belcastro Loris, Cantini Riccardo, Orsino Alessio

Licensed under the [MIT License](LICENSE.txt).