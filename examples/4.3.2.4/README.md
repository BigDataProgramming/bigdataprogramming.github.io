# Storm Programming Example: Real-time Hashtag Counting

The proposed programming example shows how Storm can be used to
analyze a stream of tweets for extracting the number of occurrences
of each hashtag. The proposed topology, composed of one spout and
two bolts, is given as follows:

- *TweetSpout* is the only data source. This spout emits into the topology a stream of tweets generated at different timestamps. 
- *SplitHashtag* receives the tuples from the spout and performs preprocessing.
Specifically, it extracts the hashtags from each tweet
and emits them into the topology for processing by the subsequent
bolt.
- *HashtagCounter* performs the counting operation using an internal
map as data structure and prints to the console the current number
of occurrences of each hashtag encountered in the stream of tuples.


The ```utils/``` folders contains some utility classes for spouts and bolts. 

> **_NOTE:_**  In this example, tweets are read from a file (```data/Twitter_dataset_USA2020.csv```), which contains text generated by Twitter users about the 2020 United States presidential election. Alternatively, following the example provided in the book, users can exploit Twitter APIs to retrieve tweets in real-time. These APIs are available in any programming language, and among them, *Twitter4j* is an unofficial open-source Java library that provides a Java-based module for conveniently accessing the Twitter Streaming API.

## How to run

The application comes with a script that automatically builds the
application and runs it on the Storm cluster.
To launch the application simply open a terminal in the _master_ container:

```bash
docker exec -ti bigdata-book-master /bin/bash
```

Then, run the following commands in the container shell:
```bash
cd /opt/examples/4.3.2.4
bash ./run.sh
```

Results will be stored in a file named ```hashtag_counts.txt``` when the topology is shut down.
